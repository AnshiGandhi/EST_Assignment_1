{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TimeGPT"
      ],
      "metadata": {
        "id": "sp21D_NJvfZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we fine-tune TimeGPT, a `sequence-to-sequence` model,  to use last three years of Crop Sown-Area (`crop_area_ha`), Irrigated Area (`irrigated_area_ha`), Actual Rainfall (`actual_rainfall_mm`), Normal Rainfall (`normal_rainfall_mm`) data to predict `water_demand_m3`.\n",
        "\n",
        "We load, clean, and preprocess the data before generating sequences for the model. We then create finetune TimeGPT and get the predictions. Later, we evaluate the model on the test set and generate metrics on the performance in terms of MAE (Mean Absolute Error), RMSE (Root Mean Square Error), and MAPE (Mean Absolute Percentage Error).\n",
        "\n",
        "Finally, we generate synthetic drought scenarios by reducing the `actual_rainfall_mm` to a fraction of its original value and getting the impact on water demand."
      ],
      "metadata": {
        "id": "UyQ4e8q4jdHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Data Preprocessing for TimeGPT"
      ],
      "metadata": {
        "id": "dJsH4CzgmiTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Libraries"
      ],
      "metadata": {
        "id": "-ZZcEk44n1F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "0B_dmbjan3ZC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Cleanup"
      ],
      "metadata": {
        "id": "fOPPjXxQn6D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAFILE_PATH = 'combined.csv'"
      ],
      "metadata": {
        "id": "gLklHrcm_Ujt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data from the CSV file and rename the columns to the format expected by TimeGPT i.e. `time` and `target`.\n",
        "\n",
        "We do not apply any scaling since TimeGPT handles that automatically."
      ],
      "metadata": {
        "id": "5OqzJ92h85Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(DATAFILE_PATH)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "dropped_cols = ['production_tonnes', 'yield_t_ha', 'irrigation_fraction']\n",
        "df = df.drop(columns=dropped_cols)\n",
        "\n",
        "# generate series id that will be used to differentiate\n",
        "df[\"series_id\"] = df[\"state\"].str.replace(\" \", \"\") + \"_\" + df[\"crop\"]\n",
        "\n",
        "df_long = df.rename(columns={\n",
        "    \"year\": \"time\",\n",
        "    \"water_demand_m3\": \"target\"\n",
        "})\n",
        "df_long[\"time\"] = pd.to_datetime(df_long[\"time\"], format=\"%Y\")\n",
        "\n",
        "# force values to be numeric\n",
        "num_cols = [\n",
        "    'crop_area_ha','irrigated_area_ha',\n",
        "    'actual_rainfall_mm','normal_rainfall_mm',\n",
        "    'cwr_m3_per_ha', 'target'\n",
        "]\n",
        "for c in num_cols:\n",
        "    df_long[c] = pd.to_numeric(df_long[c], errors='coerce')\n",
        "\n",
        "df_long = df_long.sort_values(by=[\"series_id\", \"time\"]).reset_index(drop=True)\n",
        "\n",
        "# rainfall anomaly\n",
        "df_long['rainfall_anomaly_mm'] = df_long['actual_rainfall_mm'] - df_long['normal_rainfall_mm']"
      ],
      "metadata": {
        "id": "q37tu874n806"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove the duplicate values by aggregating over the various parameters."
      ],
      "metadata": {
        "id": "XVJBRveQmuwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_long = df_long.groupby(['state','time','crop'], as_index=False).agg({\n",
        "    'crop_area_ha':'first',\n",
        "    'irrigated_area_ha':'mean',\n",
        "    'actual_rainfall_mm':'mean',\n",
        "    'normal_rainfall_mm':'mean',\n",
        "    'cwr_m3_per_ha':'mean',\n",
        "    'target':'mean',\n",
        "    'drought_flag':'max',\n",
        "    'rainfall_anomaly_mm':'mean',\n",
        "    'series_id':'first',\n",
        "})"
      ],
      "metadata": {
        "id": "pDuHZnhLhZ0y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropped_cols = ['state', 'crop']\n",
        "df_long = df_long.drop(columns=dropped_cols)"
      ],
      "metadata": {
        "id": "Dc5y0-PXkmuO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Train - Test Split"
      ],
      "metadata": {
        "id": "fatL9_MMoUg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since TimeGpt expects series to have at least 25 observations (during training), we only keep those series that have at least 35 observations. The 10 buffer observations are kept since a train-test split will be performed, which will further reduce the number of training observations."
      ],
      "metadata": {
        "id": "ZtnEOrRpAbjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 5"
      ],
      "metadata": {
        "id": "nJiu6CWkBUPK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many observations each series_id has\n",
        "counts = df_long.groupby(\"series_id\")[\"time\"].count()\n",
        "\n",
        "# Keep only series with >= 35 observations\n",
        "valid_ids = counts[counts >= 35].index\n",
        "df_long = df_long[df_long[\"series_id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "\n",
        "print(\"Remaining series:\", df_long[\"series_id\"].nunique())\n",
        "series_lengths = df_long.groupby(\"series_id\").size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfy_EtXvuOLv",
        "outputId": "6da2bebd-2d99-41e7-9bef-f8fe4509a892"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining series: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split fractions\n",
        "train_parts, test_parts = [], []\n",
        "for sid, group in df_long.groupby(\"series_id\"):\n",
        "    train_parts.append(group.iloc[:-HORIZON])\n",
        "    test_parts.append(group.iloc[-HORIZON:])\n",
        "\n",
        "train_df = pd.concat(train_parts).reset_index(drop=True)\n",
        "test_df  = pd.concat(test_parts).reset_index(drop=True)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")"
      ],
      "metadata": {
        "id": "9Jih_NIWmz9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24f2fd0-a15a-49ca-979d-ff91b9adcbb5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (2400, 10), Test shape: (235, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Fine-tuning TimeGPT and obtaining predictions"
      ],
      "metadata": {
        "id": "FEaZS4ADm-ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Libraries"
      ],
      "metadata": {
        "id": "tdOvYGrYB0gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nixtla"
      ],
      "metadata": {
        "id": "c-fTjzKRo3VC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nixtla import NixtlaClient\n",
        "\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "l-ie3ucNwj1n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Fine-tune the model"
      ],
      "metadata": {
        "id": "5A_LEPiMpJIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_COLS = [\"crop_area_ha\", \"irrigated_area_ha\", \"actual_rainfall_mm\", \"normal_rainfall_mm\", \"rainfall_anomaly_mm\", \"cwr_m3_per_ha\", \"drought_flag\"]\n",
        "CUSTOM_MODEL = \"my-custom_water_demand_model\""
      ],
      "metadata": {
        "id": "3HC0oX5Xe0n-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENSURE THAT YOUR NIXTLA API KEY IS EITHER EXPORTED AS AN ENVIRONMENT VARIABLE\n",
        "# OR THAT YOU HAVE A .env FILE CONTAINING YOUR API KEY\n",
        "nixtla = NixtlaClient()\n",
        "\n",
        "nixtla.finetune(\n",
        "    df=train_df,\n",
        "    freq='YS',\n",
        "    finetune_steps=10,\n",
        "    finetune_loss='mae',\n",
        "    finetune_depth=2,\n",
        "    time_col='time',\n",
        "    target_col='target',\n",
        "    id_col='series_id',\n",
        "    model='timegpt-1',\n",
        "    output_model_id=CUSTOM_MODEL\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dcKU5p0meFOv",
        "outputId": "5108a7df-fc8a-433b-8240-f0fadbaba61b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my-custom_water_demand_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Get Predictions from the Model"
      ],
      "metadata": {
        "id": "hkczLDOJCKXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `hist_exog_list` contains the features that are used by the model to predict the target. The model conditions on these features and then generates its predictions."
      ],
      "metadata": {
        "id": "dJrGTvs-CRvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcst = nixtla.forecast(\n",
        "    df=train_df,\n",
        "    freq='YS',\n",
        "    h=HORIZON,\n",
        "    time_col=\"time\",\n",
        "    target_col=\"target\",\n",
        "    id_col=\"series_id\",\n",
        "    hist_exog_list=FEATURE_COLS,\n",
        "    finetuned_model_id=CUSTOM_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "c4xT9oguCOgZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. TimeGPT Performance Evaluation"
      ],
      "metadata": {
        "id": "nnVfm7F-Cjaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Prepare Test Set"
      ],
      "metadata": {
        "id": "AVfWrW-4pQZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation set is generated by performing an operation similar to SQL JOIN so that we can match the predictions to the respective `series_id` and `time`. These columns are used as the keys for the JOIN. Later, `target` and `TimeGPT` columns are extracted to calculate the metrics."
      ],
      "metadata": {
        "id": "20Orfw_OCpjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval = pd.merge(\n",
        "    test_df[['series_id','time','target']],\n",
        "    fcst[['series_id','time','TimeGPT']],\n",
        "    on=['series_id','time'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "y_true = df_eval['target'].values\n",
        "y_pred = df_eval['TimeGPT'].values"
      ],
      "metadata": {
        "id": "NQapkwoEpSYK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Compute Performance Metrics"
      ],
      "metadata": {
        "id": "VLIDrQ8ApU9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the following metrics:\n",
        "\n",
        "- MAE: Mean Absolute Error (in 1e9 m^3 units)\n",
        "- RMSE: Root Mean Square Error (in 1e9 m^3 units)\n",
        "- SMAPE: Symmetric Mean Absolute Percentage Error (in %)"
      ],
      "metadata": {
        "id": "9Jc5m-QeDAIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
        "\n",
        "print(f\"MAE: {mae/1e6:.2f} (1e6 m^3)\")\n",
        "print(f\"RMSE: {rmse/1e6:.2f} (1e6 m^3)\")\n",
        "print(f\"MAPE: {mape:.2f}%\")\n",
        "print(f\"SMAPE: {smape:.2f}%\")"
      ],
      "metadata": {
        "id": "rb3YfVEepYGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309cc250-4b2d-4588-965e-bf0ef0f2bd69"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 1017.26 (1e6 m^3)\n",
            "RMSE: 2005.92 (1e6 m^3)\n",
            "MAPE: 28.58%\n",
            "SMAPE: 18.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we plot the predictions of the model against the actual values for the test set. A plot is generated for each `series_id` i.e. `state_crop` combination for the chosen horizon."
      ],
      "metadata": {
        "id": "N_sy2jfPDaoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Plot predictions\n",
        "# -------------------------------\n",
        "for sid in df_eval['series_id'].unique():\n",
        "    temp = df_eval[df_eval['series_id']==sid]\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.plot(temp['time'], temp['target'], marker='o', label='Actual')\n",
        "    plt.plot(temp['time'], temp['TimeGPT'], marker='x', label='Predicted')\n",
        "    plt.title(sid)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Water Demand (mÂ³)')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1xMDyd2M987e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Drought-scenario Evaluation"
      ],
      "metadata": {
        "id": "5DcY88dGDm9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We simulate drought / excess rainfall by varying the `actual_rainfall_mm` parameter and adjusting other parameters accordingly. We then ask the model to generate predictions for this new simulated condition."
      ],
      "metadata": {
        "id": "tyHAAgmdX6p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAINFALL_FACTOR = 0.5"
      ],
      "metadata": {
        "id": "MWf7-msfDrhP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_drought = train_df.copy()\n",
        "\n",
        "train_df_drought[\"actual_rainfall_mm\"] *= RAINFALL_FACTOR\n",
        "train_df_drought[\"rainfall_anomaly_mm\"] = (\n",
        "    train_df_drought[\"actual_rainfall_mm\"] - train_df_drought[\"normal_rainfall_mm\"]\n",
        ")\n",
        "\n",
        "# Explicitly set as int\n",
        "train_df_drought[\"drought_flag\"] = 1\n",
        "train_df_drought[\"drought_flag\"] = train_df_drought[\"drought_flag\"].astype(int)\n",
        "\n",
        "train_df_drought = train_df_drought.rename(columns={\n",
        "    \"year\": \"time\",\n",
        "    \"water_demand_m3\": \"target\"\n",
        "})\n",
        "train_df_drought[\"time\"] = pd.to_datetime(train_df_drought[\"time\"], format=\"%Y\")"
      ],
      "metadata": {
        "id": "_sUciTe-LERy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get predictions from the model in the simulated conditions."
      ],
      "metadata": {
        "id": "Q0xccETRYm5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcst_drought = nixtla.forecast(\n",
        "    df=train_df_drought,\n",
        "    freq='YS',\n",
        "    h=HORIZON,\n",
        "    time_col=\"time\",\n",
        "    target_col=\"target\",\n",
        "    id_col=\"series_id\",\n",
        "    hist_exog_list=FEATURE_COLS\n",
        ")"
      ],
      "metadata": {
        "id": "hqyW1SXTPmyF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the dataframes and compare the result with baseline."
      ],
      "metadata": {
        "id": "B1eI5wrbYrQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcst = fcst.rename(columns={\"TimeGPT\": \"Baseline\"})\n",
        "fcst_drought = fcst_drought.rename(columns={\"TimeGPT\": \"Drought\"})\n",
        "\n",
        "df_eval_drought = pd.merge(\n",
        "    fcst[['series_id','time','Baseline']],\n",
        "    fcst_drought[['series_id','time','Drought']],\n",
        "    on=['series_id','time'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "df_eval_drought[\"Impact\"] = df_eval_drought[\"Drought\"] - df_eval_drought[\"Baseline\"]\n",
        "df_eval_drought[\"Percent_Change\"] = (df_eval_drought[\"Impact\"] / df_eval_drought[\"Baseline\"]) * 100\n",
        "scale = 1e9\n",
        "df_eval_drought[[\"Baseline\",\"Drought\",\"Impact\"]] /= scale\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "print(\"All absolute values are in 1e9 m^3 units\\n\")\n",
        "display(df_eval_drought[:16])\n"
      ],
      "metadata": {
        "id": "3_1eiuzzSMrH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}