{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBNlntk6h5kD"
      },
      "source": [
        "# I. Agriculture Water Demand Prediction using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyQ4e8q4jdHO"
      },
      "source": [
        "In this section, we use an LSTM model to use last three years of Crop Sown-Area (`crop_area_ha`), Irrigated Area (`irrigated_area_ha`), Actual Rainfall (`actual_rainfall_mm`), Normal Rainfall (`normal_rainfall_mm`) data to predict `water_demand_m3`.\n",
        "\n",
        "We load, clean, and preprocess the data before generating sequences for the model. We then create Data Loaders and define the model architecture. Later, we evaluate the model on the test set and generate metrics on the performance in terms of MAE (Mean Absolute Error) and RMSE (Root Mean Square Error).\n",
        "\n",
        "Finally, we generate synthetic drought scenarios by reducing the `actual_rainfall_mm` to a fraction of its original value and getting the impact on water demand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jtld3BSkene"
      },
      "source": [
        "## A. Data Preprocessing for LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB9KB8LxgZSF"
      },
      "source": [
        "### 1. Import Libraries and Define Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0H_8NZGrgdI6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tzvcG8_4NAB0"
      },
      "outputs": [],
      "source": [
        "DATAFILE_PATH = 'combined.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzHN9IQhgPTG"
      },
      "source": [
        "### 2. Load and Clean CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e56BGrsfgGfX"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATAFILE_PATH)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "dropped_cols = ['production_tonnes', 'yield_t_ha', 'irrigation_fraction']\n",
        "df = df.drop(columns=dropped_cols)\n",
        "\n",
        "num_cols = [\n",
        "    'crop_area_ha','irrigated_area_ha',\n",
        "    'actual_rainfall_mm','normal_rainfall_mm',\n",
        "    'cwr_m3_per_ha'\n",
        "]\n",
        "for c in num_cols + ['water_demand_m3']:\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "df['year'] = pd.to_numeric(df['year'], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu5qqmyVgkxS"
      },
      "source": [
        "### 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_QauvKku0_A"
      },
      "source": [
        "We calculate the `rainfall_anomaly_mm` as the absolute difference between `actual_rainfall_mm` and `normal_rainfall_mm`. We also convert `state_id` and `crop_id` into an embedding that will also be passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SvaoFdGYgnQa"
      },
      "outputs": [],
      "source": [
        "# rainfall anomaly\n",
        "df['rainfall_anomaly_mm'] = df['actual_rainfall_mm'] - df['normal_rainfall_mm']\n",
        "num_cols.append('rainfall_anomaly_mm')\n",
        "\n",
        "# categorical encodings\n",
        "state2id = {s:i for i,s in enumerate(sorted(df['state'].unique()))}\n",
        "crop2id = {s:i for i,s in enumerate(sorted(df['crop'].unique()))}\n",
        "df['state_id'] = df['state'].map(state2id)\n",
        "df['crop_id'] = df['crop'].map(crop2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui13veQrgwEP"
      },
      "source": [
        "### 4. Handle Duplicates in the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H9bV2qS7g0rH"
      },
      "outputs": [],
      "source": [
        "collapsed_df = df.groupby(['state','year','crop'], as_index=False).agg({\n",
        "    'crop_area_ha':'first',\n",
        "    'irrigated_area_ha':'mean',\n",
        "    'actual_rainfall_mm':'mean',\n",
        "    'normal_rainfall_mm':'mean',\n",
        "    'cwr_m3_per_ha':'mean',\n",
        "    'water_demand_m3':'mean',\n",
        "    'drought_flag':'max',\n",
        "    'rainfall_anomaly_mm':'mean',\n",
        "    'state_id':'first',\n",
        "    'crop_id':'first'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKaKeKmxjhbH"
      },
      "source": [
        "### 6. Train - Validation - Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7bjbDTZ-oW06"
      },
      "outputs": [],
      "source": [
        "VAL_SIZE = 0.2\n",
        "TEST_SIZE = 0.25\n",
        "SEED = 42\n",
        "\n",
        "train_df, test_df = train_test_split(collapsed_df, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_df, val_df  = train_test_split(train_df, test_size=VAL_SIZE, random_state=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy_IGFgVhBNS"
      },
      "source": [
        "### 7. Scale the Data to avoid Magnitude Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERpWG1Z93SDJ"
      },
      "source": [
        "The target `water_demand_m3` is scaled separately from the other features to prevent data leakage to the model. The scaler is kept in memory to perform inverse transformations on the predictions generated by the model. The other feature columns are scaled together using `StandardScaler`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IECObbaZhTZH"
      },
      "outputs": [],
      "source": [
        "non_scale_cols = [\"state\",\"year\",\"crop\",\"drought_flag\",\"state_id\",\"crop_id\", \"water_demand_m3\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_df[num_cols])\n",
        "\n",
        "target_scaler = StandardScaler()\n",
        "target_scaler.fit(train_df[[\"water_demand_m3\"]])\n",
        "\n",
        "train_df[[\"water_demand_m3\"]] = pd.DataFrame(\n",
        "    target_scaler.transform(train_df[[\"water_demand_m3\"]]),\n",
        "    index=train_df.index,\n",
        "    columns=[\"water_demand_m3\"]\n",
        ")\n",
        "\n",
        "val_df[[\"water_demand_m3\"]] = pd.DataFrame(\n",
        "    target_scaler.transform(val_df[[\"water_demand_m3\"]]),\n",
        "    index=val_df.index,\n",
        "    columns=[\"water_demand_m3\"]\n",
        ")\n",
        "\n",
        "test_df[[\"water_demand_m3\"]] = pd.DataFrame(\n",
        "    target_scaler.transform(test_df[[\"water_demand_m3\"]]),\n",
        "    index=test_df.index,\n",
        "    columns=[\"water_demand_m3\"]\n",
        ")\n",
        "\n",
        "def scale_and_merge(df, scaler, scale_cols, non_scale_cols):\n",
        "    scaled = scaler.transform(df[scale_cols])\n",
        "    scaled_df = pd.DataFrame(scaled, columns=num_cols, index=df.index)\n",
        "    return pd.concat([scaled_df, df[non_scale_cols]], axis=1)\n",
        "\n",
        "train_scaled = scale_and_merge(train_df, scaler, num_cols, non_scale_cols)\n",
        "val_scaled   = scale_and_merge(val_df, scaler, num_cols, non_scale_cols)\n",
        "test_scaled  = scale_and_merge(test_df, scaler, num_cols, non_scale_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awigItLWiVRH"
      },
      "source": [
        "### 8. Generate Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcW7I-GavGnd"
      },
      "source": [
        "Here we generate the time-series that will be fed to the LSTM to give it temporal-awareness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ytu2GF4eoRb0"
      },
      "outputs": [],
      "source": [
        "CONTEXT = 3  # previous data (number of years) that the model should use to predict future demand\n",
        "HORIZON = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xH4PYeSJg9VS"
      },
      "outputs": [],
      "source": [
        "def build_sequences(df_in, group_cols=['state','crop'], time_col='year',\n",
        "                    T=CONTEXT, H=HORIZON, target_col='water_demand_m3', feature_cols=None):\n",
        "    sequences = []\n",
        "    if feature_cols is None:\n",
        "      feature_cols = [c for c in df_in.select_dtypes(include=[np.number]).columns if c != target_col]\n",
        "\n",
        "    for _, gdf in df_in.groupby(group_cols):\n",
        "        gdf = gdf.sort_values(time_col)\n",
        "        if feature_cols is None:\n",
        "            feature_cols = [c for c in gdf.select_dtypes(include=[np.number]).columns if c not in [target_col]]\n",
        "        arr_feat = gdf[feature_cols].to_numpy(dtype=float)\n",
        "        arr_target = gdf[target_col].to_numpy(dtype=float)\n",
        "        years = gdf[time_col].to_numpy()\n",
        "\n",
        "        for i in range(T, len(gdf)-H+1):\n",
        "            Xpast = arr_feat[i-T:i]\n",
        "            Yfuture = arr_target[i:i+H]\n",
        "            sequences.append({\n",
        "                \"Xpast\": Xpast,\n",
        "                \"Yfuture\": Yfuture,\n",
        "                \"start_year\": int(years[i-T]),\n",
        "                \"end_year\": int(years[i+H-1])\n",
        "            })\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3uOmPi9dxAQ",
        "outputId": "9fef7902-cdf1-49c9-eeef-4744f2c27236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train seqs: 1649, Val seqs: 272, Test seqs: 571\n"
          ]
        }
      ],
      "source": [
        "feature_cols = [\n",
        "    'state_id','crop_id','crop_area_ha','irrigated_area_ha', 'drought_flag',\n",
        "    'actual_rainfall_mm','normal_rainfall_mm','rainfall_anomaly_mm','cwr_m3_per_ha'\n",
        "]\n",
        "\n",
        "# Build sequences per split\n",
        "train_sequences = build_sequences(train_scaled, feature_cols=feature_cols)\n",
        "val_sequences   = build_sequences(val_scaled, feature_cols=feature_cols)\n",
        "test_sequences  = build_sequences(test_scaled, feature_cols=feature_cols)\n",
        "\n",
        "print(f\"Train seqs: {len(train_sequences)}, Val seqs: {len(val_sequences)}, Test seqs: {len(test_sequences)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2A7sRdnkRL_"
      },
      "source": [
        "## B. LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPVocdazkkhx"
      },
      "source": [
        "### 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c2BnwNMCknHG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAvNgKz9kpqG"
      },
      "source": [
        "### 2. Convert numpy arrays to PyTorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr14BXjnks_t",
        "outputId": "ed842467-8bab-4f6f-a730-87d21c56389e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_t shape: torch.Size([1649, 3, 9])\n",
            "y_train_t shape: torch.Size([1649, 1])\n"
          ]
        }
      ],
      "source": [
        "def sequences_to_arrays(sequences, H=1):\n",
        "    X = np.stack([s['Xpast'] for s in sequences])\n",
        "    y = np.stack([s['Yfuture'] for s in sequences])\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = sequences_to_arrays(train_sequences)\n",
        "X_val, y_val     = sequences_to_arrays(val_sequences)\n",
        "X_test, y_test   = sequences_to_arrays(test_sequences)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_t   = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_t   = torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "print(\"X_train_t shape:\", X_train_t.shape)\n",
        "print(\"y_train_t shape:\", y_train_t.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIiMoSSVkwMd"
      },
      "source": [
        "### 3. Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7w0fMAxkyL9",
        "outputId": "1a925d82-8f8d-454c-dd30-b39b02e12ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 103 Val batches: 17 Test batches: 36\n"
          ]
        }
      ],
      "source": [
        "# --- Build column index mapping once ---\n",
        "cat_cols  = ['state_id', 'crop_id']\n",
        "cont_cols = [c for c in feature_cols if c not in cat_cols]\n",
        "\n",
        "# Map feature names to indices\n",
        "col2idx = {c: i for i, c in enumerate(feature_cols)}\n",
        "cat_idx = [col2idx[c] for c in cat_cols]\n",
        "cont_idx = [col2idx[c] for c in cont_cols]\n",
        "\n",
        "# --- Helper function to extract categorical and continuous arrays ---\n",
        "def sequences_to_arrays_emb(sequences, cat_idx, cont_idx):\n",
        "    X_cat = np.stack([s['Xpast'][:, cat_idx] for s in sequences])\n",
        "    X_cont = np.stack([s['Xpast'][:, cont_idx] for s in sequences])\n",
        "    y = np.stack([s['Yfuture'] for s in sequences])\n",
        "    return X_cat, X_cont, y\n",
        "\n",
        "# Extract arrays safely\n",
        "X_train_cat, X_train_cont, y_train = sequences_to_arrays_emb(train_sequences, cat_idx, cont_idx)\n",
        "X_val_cat, X_val_cont, y_val = sequences_to_arrays_emb(val_sequences, cat_idx, cont_idx)\n",
        "X_test_cat, X_test_cont, y_test = sequences_to_arrays_emb(test_sequences, cat_idx, cont_idx)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_cat_t = torch.tensor(X_train_cat, dtype=torch.long)\n",
        "X_train_cont_t = torch.tensor(X_train_cont, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_val_cat_t = torch.tensor(X_val_cat, dtype=torch.long)\n",
        "X_val_cont_t = torch.tensor(X_val_cont, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "X_test_cat_t = torch.tensor(X_test_cat, dtype=torch.long)\n",
        "X_test_cont_t = torch.tensor(X_test_cont, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# --- Create TensorDatasets ---\n",
        "train_dataset = TensorDataset(X_train_cat_t, X_train_cont_t, y_train_t)\n",
        "val_dataset   = TensorDataset(X_val_cat_t, X_val_cont_t, y_val_t)\n",
        "test_dataset  = TensorDataset(X_test_cat_t, X_test_cont_t, y_test_t)\n",
        "\n",
        "# --- Create DataLoaders ---\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader), \"Test batches:\", len(test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryHqQ4CKk0hN"
      },
      "source": [
        "### 4. Define LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j0_yttgvlSrk"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 60\n",
        "LR = 0.0001\n",
        "DROPOUT = 0.3\n",
        "HIDDEN_DIM = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT7Og34c4LKt"
      },
      "source": [
        "The features `crop_id` and `state_id` are passed to the model as **embeddings** instead of one-hot encoding. This allows the model to understand crops and states in a higher-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5GeWKmPsk3UF"
      },
      "outputs": [],
      "source": [
        "class LSTMForecastWithEmbeddings(nn.Module):\n",
        "    def __init__(self, num_states, num_crops, emb_dim,\n",
        "                 num_cont_features, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.state_emb = nn.Embedding(num_states, emb_dim)\n",
        "        self.crop_emb  = nn.Embedding(num_crops, emb_dim)\n",
        "\n",
        "        # Total input dim = embedding dims + numeric features\n",
        "        input_dim = 2*emb_dim + num_cont_features\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x_cat, x_cont):\n",
        "        \"\"\"\n",
        "        x_cat: (batch, T, 2) -> state_id, crop_id\n",
        "        x_cont: (batch, T, num_cont_features)\n",
        "        \"\"\"\n",
        "        # Embeddings\n",
        "        state_emb = self.state_emb(x_cat[:,:,0].long())\n",
        "        crop_emb  = self.crop_emb(x_cat[:,:,1].long())\n",
        "\n",
        "        # Concatenate embeddings with continuous features\n",
        "        x = torch.cat([state_emb, crop_emb, x_cont], dim=-1)\n",
        "\n",
        "        # LSTM\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4JpRBUZk7ed"
      },
      "source": [
        "### 5. Instantiate and Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZNAgonj4-tb"
      },
      "source": [
        "We are using `dropout` and `learning_rate` scheduling to avoid overfitting. Finally, we also plot the training/validation loss curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0QC4Smsk-p2"
      },
      "outputs": [],
      "source": [
        "num_states = len(state2id)\n",
        "num_crops  = len(crop2id)\n",
        "num_cont_features = len(cont_cols)\n",
        "\n",
        "model = LSTMForecastWithEmbeddings(\n",
        "    num_states=num_states,\n",
        "    num_crops=num_crops,\n",
        "    emb_dim=3,\n",
        "    num_cont_features=num_cont_features,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    output_dim=1,\n",
        "    num_layers=2,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "# --- Loss and optimizer ---\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.8)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for xb_cat, xb_cont, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        yb = yb.view(-1, 1)\n",
        "        pred = model(xb_cat, xb_cont)\n",
        "        loss = criterion(pred, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * xb_cat.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for xb_cat, xb_cont, yb in val_loader:\n",
        "            yb = yb.view(-1, 1)\n",
        "            pred = model(xb_cat, xb_cont)\n",
        "            loss = criterion(pred, yb)\n",
        "            val_loss += loss.item() * xb_cat.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Plotting Loss Curve\n",
        "# -------------------------------\n",
        "print()\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, EPOCHS+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, EPOCHS+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Train/Validation Loss Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91t9n4hykjOS"
      },
      "source": [
        "## C. Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGy0LVGZlDG9"
      },
      "source": [
        "### 1. Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T0litDX2ktF3"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCQveACc5bfG"
      },
      "source": [
        "We generate predictions from the model and perform an inverse transformation to get the values in `m^3` units. We calculate per-crop `MAE: Mean Average Error` and `RMSE: Root Mean Square Error` and overall `SMAPE: Symmetric Mean Absolute Percentage Error` to evaluate the performance of the model. This gives us a baseline to evaluate the performance of `sequence-to-sequence` models such as `TimeGPT`.\n",
        "\n",
        "We also plot the predictions of the model against actuals to visualise the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_TQi3dhki6x"
      },
      "outputs": [],
      "source": [
        "all_preds, all_targets, all_crops = [], [], []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for xb_cat, xb_cont, yb in test_loader:\n",
        "        yb = yb.view(-1, 1)\n",
        "        pred = model(xb_cat, xb_cont)\n",
        "\n",
        "        # Collect categorical info\n",
        "        crop_ids = xb_cat[:, -1, 1].cpu().numpy()\n",
        "        all_crops.append(crop_ids)\n",
        "\n",
        "        all_preds.append(pred.cpu().numpy())\n",
        "        all_targets.append(yb.cpu().numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds, axis=0)\n",
        "all_targets = np.concatenate(all_targets, axis=0)\n",
        "all_crops = np.concatenate(all_crops, axis=0)\n",
        "\n",
        "# Inverse transform predictions and targets to original units\n",
        "pred_real = target_scaler.inverse_transform(all_preds).ravel()\n",
        "targets_real = target_scaler.inverse_transform(all_targets).ravel()\n",
        "\n",
        "# Calculate SMAPE\n",
        "smape = 100 * np.mean(2 * np.abs(pred_real - targets_real) / (np.abs(pred_real) + np.abs(targets_real)))\n",
        "\n",
        "# Compute MAE and RMSE per crop\n",
        "results = []\n",
        "for crop_id, crop_name in crop2id.items():\n",
        "    mask = all_crops == crop_name\n",
        "    if np.sum(mask) == 0:\n",
        "        continue\n",
        "    crop_mae = mean_absolute_error(targets_real[mask], pred_real[mask])\n",
        "    crop_rmse = np.sqrt(mean_squared_error(targets_real[mask], pred_real[mask]))\n",
        "    results.append((crop_id, crop_mae, crop_rmse))\n",
        "\n",
        "crop_metrics_df = pd.DataFrame(results, columns=['Crop', 'MAE_m3', 'RMSE_m3'])\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "display(crop_metrics_df)\n",
        "print(f\"\\n\\nSMAPE: {smape: .4f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AniRHteBk6jE"
      },
      "source": [
        "### 2. Plot Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbkRn6tqk6FO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "plt.plot(targets_real, label='True')\n",
        "plt.plot(pred_real, label='Predicted')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Water Demand (mÂ³)')\n",
        "plt.title('Predicted vs True Water Demand on Test Set')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgMqnO6Hk_zO"
      },
      "source": [
        "### 3. Simulate Drought and obtain Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "faHcRRq1lEAZ"
      },
      "outputs": [],
      "source": [
        "RAINFALL_FACTOR = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fuK3fsJqll_x"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "test_dataset  = TensorDataset(X_test_cat_t, X_test_cont_t, y_test_t)\n",
        "test_loader_random = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "sim_X_cat, sim_X_cont, sim_y = next(iter(test_loader_random))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GA3_FHJbqe-"
      },
      "outputs": [],
      "source": [
        "sim_X_cat, sim_X_cont, sim_y = next(iter(test_loader_random))\n",
        "\n",
        "# --- Baseline prediction (normal rainfall) ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_baseline = model(sim_X_cat, sim_X_cont)\n",
        "\n",
        "# --- Simulate drought by reducing rainfall ---\n",
        "sim_X_cont_drought = sim_X_cont.clone()\n",
        "\n",
        "rain_idx   = cont_cols.index('actual_rainfall_mm')\n",
        "normal_idx = cont_cols.index('normal_rainfall_mm')\n",
        "anom_idx   = cont_cols.index('rainfall_anomaly_mm')\n",
        "flag_idx   = cont_cols.index('drought_flag')\n",
        "\n",
        "# Apply drought factor\n",
        "sim_X_cont_drought[:, :, rain_idx] *= RAINFALL_FACTOR\n",
        "\n",
        "# Recompute anomaly (actual - normal)\n",
        "sim_X_cont_drought[:, :, anom_idx] = (\n",
        "    sim_X_cont_drought[:, :, rain_idx] - sim_X_cont_drought[:, :, normal_idx]\n",
        ")\n",
        "\n",
        "# Set drought flag\n",
        "sim_X_cont_drought[:, :, flag_idx] = 1.0\n",
        "\n",
        "\n",
        "# --- Drought prediction ---\n",
        "with torch.no_grad():\n",
        "    pred_drought = model(sim_X_cat, sim_X_cont_drought)\n",
        "\n",
        "# --- Inverse transform predictions ---\n",
        "pred_baseline_real = target_scaler.inverse_transform(pred_baseline.cpu().numpy()).ravel()\n",
        "pred_drought_real = target_scaler.inverse_transform(pred_drought.cpu().numpy()).ravel()\n",
        "\n",
        "# --- Compute impact ---\n",
        "impact = pred_drought_real - pred_baseline_real\n",
        "percent_change = (impact / pred_baseline_real) * 100\n",
        "\n",
        "# --- Inverse mapping dictionaries ---\n",
        "id2state = {v: k for k, v in state2id.items()}\n",
        "id2crop  = {v: k for k, v in crop2id.items()}\n",
        "\n",
        "# --- Print results with crop and state names ---\n",
        "results_table = []\n",
        "\n",
        "for i in range(len(pred_baseline_real)):\n",
        "    state_name = id2state[int(sim_X_cat[i, 0, 0])]\n",
        "    crop_name  = id2crop[int(sim_X_cat[i, 0, 1])]\n",
        "\n",
        "    results_table.append({\n",
        "        \"State\": state_name,\n",
        "        \"Crop\": crop_name,\n",
        "        \"Baseline\": pred_baseline_real[i] / 1e9,\n",
        "        \"Prediction\": pred_drought_real[i] / 1e9,\n",
        "        \"Impact\": impact[i] / 1e9,\n",
        "        \"Change (%)\": percent_change[i]\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results_table)\n",
        "\n",
        "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
        "print(\"All absolute values in 1e9 m^3\")\n",
        "display(df_results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
